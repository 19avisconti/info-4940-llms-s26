{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e539da-9d8a-40ae-898b-91d850b85b65",
   "metadata": {},
   "source": [
    "## Practice Calculating Minimal and Maximal Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebeb388-b7df-444f-8340-86ced556d757",
   "metadata": {},
   "source": [
    "<p style=\"text-align: left; border-radius: 5px; background-color: #e3f3fb; color: #2a8bc6; padding: 10px 10px; border: 1px solid #00008B;\" role=\"alert\"><strong>Note:</strong> \n",
    "For optimal viewing, it is recommended that you click the fullscreen button in the lower-right corner of your coding window. General information on how to work with Jupyter Notebooks is contained in a collapsable panel to the right of every Jupyter Notebook. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf81eec-bcb9-4237-b0d5-d111260cf872",
   "metadata": {},
   "source": [
    "In the previous video, you explored how uncertainty relates to probability distributions using the example of fair and unfair dice. Intuitively, the more evenly probabilities are spread, the more uncertain we are about the outcome. This uncertainty can be measured using **entropy**, a key concept in information theory and language modeling.\n",
    "\n",
    "In this activity, you’ll calculate the entropy of different discrete distributions, interpret what it means, and relate entropy to the concept of the “effective number” of likely outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216988c-7094-4736-842e-561f06b9b3c1",
   "metadata": {},
   "source": [
    "## Your Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d317d1-524c-4c75-9206-a742e5a7e7bf",
   "metadata": {},
   "source": [
    "To practice working with entropy, you will need to complete the following tasks.\n",
    "\n",
    "* [Task 1 of 7: Define a Function for Entropy](#Task-1-of-7:-Define-a-Function-for-Entropy)\n",
    "\n",
    "* [Task 2 of 7: Calculate Entropy for a Fair Die](#Task-2-of-7:-Calculate-Entropy-for-a-Fair-Die)\n",
    "\n",
    "* [Task 3 of 7: Calculate Entropy for an Unfair Die](#Task-3-of-7:-Calculate-Entropy-for-an-Unfair-Die)\n",
    "\n",
    "* [Task 4 of 7: Calculate Entropy for a Somewhat Unfair Die](#Task-4-of-7:-Calculate-Entropy-for-a-Somewhat-Unfair-Die)\n",
    "\n",
    "* [Task 5 of 7: Find the Equivalent Number of Outcomes](#Task-5-of-7:-Find-the-Equivalent-Number-of-Outcomes)\n",
    "\n",
    "* [Task 6 of 7: Experiment With Uncertainty](#Task-6-of-7:-Experiment-With-Uncertainty)\n",
    "\n",
    "* [Task 7 of 7: Save and Download This Notebook](#Task-7-of-7:-Save-and-Download-This-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e8b81-3497-4cc9-87ec-c6aca29bf34a",
   "metadata": {},
   "source": [
    "## Task 1 of 7: Define a Function for Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e558d62-9694-44ec-b20a-27278b41fcd0",
   "metadata": {},
   "source": [
    "**Entropy** quantifies uncertainty in a probability distribution. The entropy of a discrete probability distribution $p_1, p_2, \\ldots, p_n$\n",
    " is defined as:\n",
    "$$\n",
    "H = -\\sum_{i=1}^{n} p_i \\log p_i\n",
    "$$\n",
    "The higher the entropy, the more unpredictable the outcome. For each possible event, you multiply its probability by the negative log of that probability and sum the terms. If an event is impossible (probability is zero), it doesn't contribute to the sum, becuase $0 \\times \\log0$ is treated as zero. \n",
    "\n",
    "Fun fact: The entropy function is usually denoted with $H$, due to a series of references going back to the 19th century, but nobody knows why it originally started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e0721-6084-49d0-809d-fcadf4f7667d",
   "metadata": {},
   "source": [
    "The function in the code cell below calcuates the entropy of a probability distribution `probs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ce1cf-5fe8-4953-b04e-68b48f2a8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(probs):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a discrete probability distribution.\n",
    "    probs: array-like of probabilities (must sum to 1)\n",
    "    Returns: entropy (using natural log)\n",
    "    \"\"\"\n",
    "    probs = np.asarray(probs)\n",
    "    \n",
    "    # Remove zero probabilities to avoid log(0)\n",
    "    nonzero_probs = probs[probs > 0]\n",
    "    \n",
    "    return -np.sum(nonzero_probs * np.log(nonzero_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ebf3c-d53b-4cd6-aaee-e1342d3e3574",
   "metadata": {},
   "source": [
    "## Task 2 of 7: Calculate Entropy for a Fair Die"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1ef1f-dcba-4d33-b0c8-be2002d913f9",
   "metadata": {},
   "source": [
    "Here we simulate a fair 6-sided die. Each face is equally likely, so we set the probability for each to 1/6. This is the situation of **maximal uncertainty**: you have no idea which side will come up next.\n",
    "\n",
    "You can use algebra to rearrange the entropy function and show that the entropy for any uniform distribution is the logarithm of the number of possible outcomes. You can also use calculus to show that the uniform distribution is the highest possible entropy.\n",
    "\n",
    "Use the code below to show that the entropy of a fair 6-sided die is $\\log 6$. This value represents the highest possible uncertainty for six outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929b618-e991-483a-b814-aa63cd50cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair six-sided die: each side has probability 1/6\n",
    "fair_probs = np.ones(6) / 6\n",
    "print(\"Probabilities for a fair die:\", fair_probs)\n",
    "\n",
    "H_fair = entropy(fair_probs)\n",
    "print(\"Entropy for fair die:\", H_fair)\n",
    "print(\"log(6):\", np.log(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55160571-6bba-4ba0-ba3d-dc3ac7331b5a",
   "metadata": {},
   "source": [
    "## Task 3 of 7: Calculate Entropy for an Unfair Die"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e34b1f-e7f1-45ae-aefc-e0c724105add",
   "metadata": {},
   "source": [
    "Now consider the opposite scenario: a die that always lands on side 6. The probability is 1 for side 6 and 0 for all others. There is zero uncertainty in this case. You know the outcome with complete confidence. What do you expect entropy to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5461c-d3c6-4f9c-a4af-42bfb661adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfair die: always lands on side 6\n",
    "unfair_probs = np.array([0, 0, 0, 0, 0, 1])\n",
    "print(\"Probabilities for a completely unfair die:\", unfair_probs)\n",
    "\n",
    "H_unfair = entropy(unfair_probs)\n",
    "print(\"Entropy for unfair die:\", H_unfair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b903319-e3f4-413b-a0bf-c4e70ffb90fd",
   "metadata": {},
   "source": [
    "## Task 4 of 7: Calculate Entropy for a Somewhat Unfair Die"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce21d11-695f-4e95-b157-d38d66e31a8f",
   "metadata": {},
   "source": [
    "Here’s a situation in between: the die is “biased” so side 6 is twice as likely as any other (probability 2/7 for side 6, 1/7 for the others). You have some information about the likely result, but not complete certainty. The entropy falls between 0 and $\\log6$, reflecting this partial uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a745473-d692-4df5-9d3a-db535ae78f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfair die: side 6 is twice as likely as each other side \n",
    "# (probabilities: 1/7 for sides 1–5, 2/7 for side 6)\n",
    "somewhat_unfair_probs = np.array([1/7, 1/7, 1/7, 1/7, 1/7, 2/7])\n",
    "print(\"Probabilities for a somewhat unfair die:\", somewhat_unfair_probs)\n",
    "\n",
    "H_somewhat = entropy(somewhat_unfair_probs)\n",
    "print(\"Entropy for somewhat unfair die:\", H_somewhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b342cc-197f-49d9-89dc-27b01ca9a37e",
   "metadata": {},
   "source": [
    "## Task 5 of 7: Find the Equivalent Number of Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50762123-9bdf-4750-98fa-e1e7616eec21",
   "metadata": {},
   "source": [
    "While entropy is a powerful measure of uncertainty, its value can sometimes feel abstract. To make entropy more intuitive, we can convert it back into a simple, countable quantity: the **equivalent number of outcomes**. <b>Note:</b> this is sometimes called the \"effective number of outcomes.\"\n",
    "\n",
    "The equivalent number of outcomes tells you, \"If all outcomes were equally likely, how many would there be to obtain the current uncertainty?\" In other words, it answers the question: \"How many sides would a fair die need to have to have the same entropy as the distribution we are measuring?\"\n",
    "\n",
    "Mathematically, the effective number is defined as the exponential of the entropy:\n",
    "$$Equivalent Number=\\exp(H)$$\n",
    "where \n",
    "$H$ is the entropy (using the natural logarithm). It reflects the *spread* or *uncertainty* of the whole distribution.\n",
    "\n",
    "**Why Is This Useful?**\n",
    "- The equivalent number provides an easy way to compare distributions of different shapes and sizes.\n",
    "- It allows you to describe the degree of uncertainty in a way that is immediately interpretable and intuitive.\n",
    "- In language modeling and information theory, it helps us understand and communicate how \"broad\" or \"narrow\" a model’s predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9936d-2f2e-45f2-ac9c-ac21d4eb4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_number(H):\n",
    "    return np.exp(H)\n",
    "\n",
    "print(\"Equivalent number for fair die with 6 sides:\", equivalent_number(H_fair))\n",
    "print(\"Equivalent number for unfair die that always lands on side 6:\", equivalent_number(H_unfair))\n",
    "print(\"Equivalent number for somewhat unfair die:\", equivalent_number(H_somewhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ce046-eb8a-4d4c-a946-afcd983974f7",
   "metadata": {},
   "source": [
    "**Why was the equivalent number of outcomes for a fair 6-sided die ~6?**  \n",
    "For a fair 6-sided die, the entropy is $log(6)$. The equivalent number is: $exp(log(6))=6$  \n",
    "So there are six equally likely outcomes.  \n",
    "\n",
    "**For a certain outcome, such as an unfair die that always lands on one side**, the entropy is 0. Therefore the equivalent number is: $exp(0)=1$  \n",
    "This means there is only one possible outcome, and no uncertainty. \n",
    "\n",
    "**The entropy for a biased die was ~1.75**. If some outcomes are more likely than others, the equivalent number will be somewhere between 1 and 6. In the example above, where one side is more likely than the rest, the **equivalent number is just a little less than 6 (5.74 in this case)**. With a more biased die where one or two outcomes are significantly more likely, you would see a lower equivalent outcome number. The effective number decreases when most probability is concentrated in a small number of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d46ab-230b-4963-8ea3-7cf4ad7a847b",
   "metadata": {},
   "source": [
    "## Task 6 of 7: Experiment With Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810e058-0a5e-41f8-9353-05d75f0101fc",
   "metadata": {},
   "source": [
    "Now, play with different probability distributions. Try making the die even more unfair or splitting the probability between two sides. Watch how entropy and the equivalent number change. Do these numbers agree with your gut sense of uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0c6bd-0133-4739-8f20-96353c627ce1",
   "metadata": {},
   "source": [
    "## Task 7 of 7: Save and Download This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5c6b6-a2c1-4235-bee0-05d94f7e1932",
   "metadata": {},
   "source": [
    "You are encouraged to save and download a copy of this notebook.\n",
    "\n",
    "To save the notebook, in the top left corner of this Jupyter Lab environment, select *File > Save All*.\n",
    "\n",
    "To download the notebook to your own computer, you can either:\n",
    "- Select *File > Download* to download the notebook in its native .ipynb file format.\n",
    "- Select *File > Save and Export Notebook As* to save the notebook in a file format of your choosing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
