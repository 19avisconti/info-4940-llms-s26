{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4cfcf92-c0b6-4198-b65c-db68fc6bb338",
   "metadata": {},
   "source": [
    "# How Likely Is a Sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff7da3-a6d1-437f-b417-d926e46f5a91",
   "metadata": {},
   "source": [
    "## Your Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35930ca4-2158-468a-89e4-bfac99b06f75",
   "metadata": {},
   "source": [
    "In this activity, you will examine how a language model assigns probabilities to each token in an input sequence and how those probabilities combine to determine the likelihood of the entire sequence. You will explore both conditional probability and log probability to see why working in log space is essential for numerical stability and meaningful interpretation.\n",
    "\n",
    "In this activity, you'll complete the following tasks:\n",
    "\n",
    "* [Task 1 of 9: Choose a Model](#Task-1-of-9:-Choose-a-Model)\n",
    "\n",
    "* [Task 2 of 9: Load the Model and Tokenizer](#Task-2-of-9:-Load-the-Model-and-Tokenizer)\n",
    "  \n",
    "* [Task 3 of 9: Prepare a Prompt and Tokenize It](#Task-3-of-9:-Prepare-a-Prompt-and-Tokenize-It)\n",
    "\n",
    "* [Task 4 of 9: Run a Forward Pass of the Language Model](#Task-4-of-9:-Run-a-Forward-Pass-of-the-Language-Model)\n",
    "\n",
    "* [Task 5 of 9: Convert Logits to Probabilities With Softmax](#Task-5-of-9:-Convert-Logits-to-Probabilities-With-Softmax)\n",
    "\n",
    "* [Task 6 of 9: Define a Helper Barplot Function](#Task-6-of-9:-Define-a-Helper-Barplot-Function)\n",
    "  \n",
    "* [Task 7 of 9: Calculate the Conditional Probability of the Overall Sentence](#Task-7-of-9:-Calculate-the-Conditional-Probability-of-the-Overall-Sentence)\n",
    " \n",
    "* [Task 8 of 9: Transform Probabilities To Make Them Easier to Work With](#Task-8-of-9:-Transform-Probabilities-To-Make-Them-Easier-to-Work-With)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f43f98-5d84-49b3-beac-55edaae221d6",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Run the code cell below to import the packages you will use in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79751e1d-5e8e-4564-a119-bf8110e11029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508e139-991c-4aa5-b009-85ea68fecf89",
   "metadata": {},
   "source": [
    "## Task 1 of 9: Choose a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c596714-013e-47af-b179-0f97b611090d",
   "metadata": {},
   "source": [
    "Navigate to the [Hugging Face Hub website](https://huggingface.co/models) and choose a text generation model in the 100M–1B parameter range.\n",
    "\n",
    ">**Important:** Choose a smaller-sized model (fewer than 1B parameters) to ensure it runs smoothly.\n",
    "If the model doesn’t load properly or you encounter errors, try selecting a different one. Anyone can upload models to the Hugging Face Hub, and some may not work as expected.\n",
    "If you continue to have trouble, use `gpt2`, a reliable model that should run without issues.\n",
    "\n",
    "**In the code cell below, complete the code to specify the model you'll use.** Replace `YOUR MODEL NAME HERE` with the model you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35bda8f9-b2fd-4aff-8440-1bce9ec81510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look around on the Hugging Face Hub for an interesting model!\n",
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc417e1-f5ab-4508-8d40-e6d2f05a99e6",
   "metadata": {},
   "source": [
    "## Task 2 of 9: Load the Model and Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8d2df-a236-4357-9b9a-ef6f4c6644c1",
   "metadata": {},
   "source": [
    "Run the code cell below to load your chosen model and its associated tokenizer.\n",
    "\n",
    "Be patient; it may take time to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979eb8c6-93b0-4ee8-be5e-a2705522bc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734b003be42743efb1dfe0b6ae19d143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6d75c4b4db4b2aa4e1eeb3837daab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef843c1020a8439eac53ab8a0fccb4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a34b537d4d47d189dfd3add6f30434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7989b577f214ef5aa4bee2531ea901e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ae61078e6145679e46d0e4a77768a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46018b0a63174d949315790b41e218f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d4f899746d49e685979f4151a7dfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here!\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)               # Load the tokenizer  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)                   # Load the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b3a80-a8cf-497b-a3ea-4e8a2c9bbf4e",
   "metadata": {},
   "source": [
    "## Task 3 of 9: Prepare a Prompt and Tokenize It"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e229ec-2e8d-413d-8560-0d5a7518a88a",
   "metadata": {},
   "source": [
    "Compose a prompt that is a complete sentence. Replace `YOUR PROMPT HERE` with your prompt. \n",
    "\n",
    "After you've defined your prompt, run the code cell below to tokenize the prompt and convert the string prompt to a PyTorch tensor containing token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c56f2f-3d8a-48d3-aa60-f53d33a8714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "prompt = \"Where should I go to eat in Ithaca? \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")  # Tokenize the prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4ccec-a33a-4589-813a-3f20d640ce8a",
   "metadata": {},
   "source": [
    "## Task 4 of 9: Run a Forward Pass of the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558173f-7330-4b02-b793-a9b894c9de00",
   "metadata": {},
   "source": [
    "The first step in determining the probability of an input sequence is computing the next-token probabilities, because the model builds up the likelihood of a sentence one token at a time, always conditioning on the words that came before.\n",
    "\n",
    "The next code cell runs a forward pass of the language model on your input sequence and returns the raw prediction scores, called logits. Remember, logits are the raw scores for every possible next-token given the input prompt (context), before they are converted into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ded8de6-80d6-450a-a058-c501462de540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b2d0a-cdb4-4bc5-9df2-78d3b7156dff",
   "metadata": {},
   "source": [
    "## Task 5 of 9: Convert Logits to Probabilities With Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c121f-a1f5-4662-9fdb-e7ab18fec9de",
   "metadata": {},
   "source": [
    "Run the code cell below to apply the `softmax()` function to the logits. This converts them into probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393a10dc-0071-4aab-a798-5d25b165ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 50257])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs[\"logits\"][0]                           # Get logits from your raw model outputs \n",
    "probabilities = F.softmax(logits, dim=1)                    # Use the softmax function to convert logits into probabilities\n",
    "probabilities.shape                    # Check out the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee07bed-6eaf-4168-b276-58ba10bffd16",
   "metadata": {},
   "source": [
    "What does the output mean? It is a matrix of probabilities with shape `(sequence_length, vocab_size)`.\n",
    "> The first dimension is the **sequence length** &mdash; the number of tokens in the input after tokenization.\n",
    "> The second dimension is the **vocabulary size** &mdash; the number of possible tokens the model can choose from at each position.\n",
    "> \n",
    "Essentially, each row corresponds to one token position in the input sequence and contains a probability distribution over all possible next-tokens in the vocabulary (the set of probabilities sum to 1). In other words, for each position, the model assigns a probability to every possible next-token in its vocabulary, given all the previous tokens in the input up to that position.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047299b-0327-4152-88f7-418b44f8e523",
   "metadata": {},
   "source": [
    "## Task 6 of 9: Define a Helper Barplot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea82ab-08e8-455f-91a8-45938b6f9018",
   "metadata": {},
   "source": [
    "Run the code cell below to define a function that translates numbers into printable bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d17abed-0473-4a4f-8eb0-5329f4fa794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [ \" \", chr(9615), chr(9614), chr(9613), chr(9612), chr(9611), chr(9610), chr(9609), chr(9608) ]\n",
    "\n",
    "def bar(x, width=15):\n",
    "    full, remainder = divmod(int(x * 8 / (1.0 / width)), 8)\n",
    "    bar_string = '█' * full + fractions[remainder]\n",
    "    return bar_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29a89a-4c95-4786-9f04-7f943ad18bd6",
   "metadata": {},
   "source": [
    "## Task 7 of 9: Calculate the Conditional Probability of the Overall Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8eb2c-7e2e-4eac-b2cc-fe65b12cd59a",
   "metadata": {},
   "source": [
    "### How do you calculate the probability of the overall sentence? Multiply the conditional probability of each word!\n",
    "\n",
    "We can take the matrix and calculate the probability of the specific input sequence by selecting the probability assigned to each token in the sequence, and multiplying them together.\n",
    "\n",
    "The *joint probability* of the full sequence is the product of the *conditional probabilities* of each word. For example, in the phrase \"I like to\"...\n",
    "\n",
    "$ P(I, like, to) = P(I) P(like | I) P(to | I, like) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1029ab7-ba6f-4972-8e5a-e1552024a231",
   "metadata": {},
   "source": [
    "Run the code cell to display a table showing each token in the sequence, its probability from the model, the probability sequence probability up to that point, and a visual probability bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bdbdc64-a9e6-424e-bda6-0901758ce128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob            Token ID   Token Prob   Decoded Token        Probability Bar\n",
      "0.001195105026  815        0.0012       ' should'             \n",
      "0.000361329408  314        0.3023       ' I'                 ███████████████ \n",
      "0.000033337789  467        0.0923       ' go'                ████▌\n",
      "0.000007066371  284        0.2120       ' to'                ██████████▌\n",
      "0.000000035618  4483       0.0050       ' eat'               ▎\n",
      "0.000000000879  287        0.0247       ' in'                █▏\n",
      "0.000000000001  314        0.0006       ' I'                  \n",
      "0.000000000000  400        0.0486       'th'                 ██▍\n",
      "0.000000000000  22260      0.9947       'aca'                █████████████████████████████████████████████████▋\n",
      "0.000000000000  30         0.4233       '?'                  █████████████████████▏\n",
      "0.000000000000  220        0.0013       ' '                   \n"
     ]
    }
   ],
   "source": [
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "# Print header\n",
    "print(\"{:<15s} {:<10s} {:<12s} {:<20s} {}\".format(\n",
    "    \"Prob\", \"Token ID\", \"Token Prob\", \"Decoded Token\", \"Probability Bar\"\n",
    "))\n",
    "\n",
    "probability = 1.0\n",
    "for position in range(1, seq_len):\n",
    "    word_id = inputs[\"input_ids\"][0, position]\n",
    "    token_prob = probabilities[(position - 1), word_id].item()\n",
    "    probability *= token_prob\n",
    "    \n",
    "    print(\"{:<15.12f} {:<10d} {:<12.4f} {:<20s} {}\".format(\n",
    "        probability,\n",
    "        word_id.item(),\n",
    "        token_prob,\n",
    "        \"'\" + tokenizer.decode(word_id) + \"'\",\n",
    "        bar(token_prob, 50)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528476f1-c67e-45fb-8594-0c492691c836",
   "metadata": {},
   "source": [
    "**Take Note**:\n",
    "- What is the probability for the last token in your prompt? **Probability tends to get *very* small very quickly.** Multiplying many probabilities together shrinks the probability *exponentially*. This is normal for sequence probabilities.\n",
    "\n",
    "- Are there any **single low-probability tokens that sharply reduce the total probability** of the full sequence?\n",
    "\n",
    "- Compare probability bars across tokens. **Tiny or empty bars mean the model was *very uncertain* about that token**; large bars mean higher probability (model confidence).\n",
    "\n",
    "- **Unlikely sequences can still be meaningful**. Just because the probability is tiny, it doesn't mean the model thinks the text is \"wrong\"; natural language is full of low-probability token combinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df5076-8d51-4dff-92ed-a4fe0cd72db5",
   "metadata": {},
   "source": [
    "## Task 8 of 9: Transform Probabilities To Make Them Easier to Work With"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb6369-676e-4cc0-ba26-85eb7dc00214",
   "metadata": {},
   "source": [
    "Calculating the probability of a sentence by multiplying token probabilities results in values that quickly become extremely small, due to the multiplication of many numbers less than 1.  \n",
    "\n",
    "There is a way to avoid this: \n",
    "\n",
    "### Add log probabilities instead of multiplying probabilities.  \n",
    "\n",
    "By summing the log probabilities instead, we avoid numerical underflow and can track the sequence likelihood without it shrinking exponentially.\n",
    "\n",
    "For example:\n",
    "> If `\"I like to\"` is tokenized into three tokens, then the probability of the whole sequence is:  \n",
    "> $\n",
    " P(I, like, to) = P(I) P(like | I) P(to | I, like)\n",
    " $\n",
    ">\n",
    ">Taking the logarithm, and using the property $(log(ab) = \\log(a) + \\log(b))$, we get:  \n",
    ">\n",
    ">$\n",
    "\\log P(\\text{I, like, to}) = \\log(P(I)) + \\log(P(like | I)) + \\log(P(to | I, like))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a9f6a-82de-412c-a3c5-df07a9b0c9e4",
   "metadata": {},
   "source": [
    "Run the code to display the running sum log probability for the sequence along with a visual probability bar for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a389d6af-79a5-4096-945b-25589fd784b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogP            Token ID   LogP(Token)  Decoded Token        Probability Bar\n",
      "-6.7            815        -6.7295      ' should'             \n",
      "-7.9            314        -1.1962      ' I'                 ███████████████ \n",
      "-10.3           467        -2.3831      ' go'                ████▌\n",
      "-11.9           284        -1.5513      ' to'                ██████████▌\n",
      "-17.2           4483       -5.2902      ' eat'               ▎\n",
      "-20.9           287        -3.7023      ' in'                █▏\n",
      "-28.2           314        -7.3936      ' I'                  \n",
      "-31.3           400        -3.0232      'th'                 ██▍\n",
      "-31.3           22260      -0.0053      'aca'                █████████████████████████████████████████████████▋\n",
      "-32.1           30         -0.8597      '?'                  █████████████████████▏\n",
      "-38.8           220        -6.6418      ' '                   \n"
     ]
    }
   ],
   "source": [
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "# Print header\n",
    "print(\"{:<15s} {:<10s} {:<12s} {:<20s} {}\".format(\n",
    "    \"LogP\", \"Token ID\", \"LogP(Token)\", \"Decoded Token\", \"Probability Bar\"\n",
    "))\n",
    "\n",
    "log_probability = 0.0\n",
    "for position in range(1, seq_len):\n",
    "    word_id = inputs[\"input_ids\"][0, position]\n",
    "\n",
    "    token_prob = probabilities[(position-1), word_id].item()\n",
    "    token_logprob = np.log(token_prob)\n",
    "    log_probability += token_logprob\n",
    "    \n",
    "    print(\"{:<15.1f} {:<10d} {:<12.4f} {:<20s} {}\".format(\n",
    "        log_probability,\n",
    "        word_id.item(),\n",
    "        token_logprob,\n",
    "        \"'\" + tokenizer.decode(word_id) + \"'\",\n",
    "        bar(token_prob, 50)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3768fd-4c27-43a3-b579-63f6804e5c98",
   "metadata": {},
   "source": [
    "**Take Note:**\n",
    "- **Log probabilities are negative values.** A high-confidence token with $P$ close to 1 will have a log probability near 0. A low-confidence token with $P$ close to 0 will have a large negative log.\n",
    "- **Log probability decreases steadily.** Since each token's log probability is added, the log probability will keep getting more negative as the sequence grows. Compare this to the probability from Task 8.\n",
    "- Even for long sequences, the numbers remain within a normal range for floating-point computation and there is **no numerical overflow**.\n",
    "- **Differences between tokens are easier to interpret.** In log space, large negative jumps indicate particularly unlikely tokens relative to the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815cf6c-30e4-4461-811d-9b2128c522f7",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "In this exercise, we explored how to track a model’s confidence in generating each token of a sequence.  \n",
    "We first calculated **probabilities** by multiplying each token’s probability and saw how these values quickly became extremely small due to the multiplication of many numbers less than 1.  \n",
    "\n",
    "To address this, we switched to **log probabilities**, which transform multiplication into addition. This not only avoids numerical underflow but also makes it easier to interpret token‑level contributions to the overall sequence likelihood.  \n",
    "\n",
    "**Key Takeaway:**  \n",
    "> Working in log space is the standard approach in NLP for measuring sequence likelihood, because it is numerically stable and more interpretable than raw probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-4940-llms-s26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
