{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68927044",
   "metadata": {},
   "source": [
    "## NAME(s):               \n",
    "## NETID(s): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932616f7",
   "metadata": {},
   "source": [
    "# Homework 01: Neural Networks\n",
    "This assignment walks you through the basics of neural network construction. You may work in groups of up to 3 people. \n",
    "\n",
    "The overall goal here is to give you a better understanding of neural networks without reliance on other packages. You'll basically be building an extremely simplified version of some Pytorch layers/functions. \n",
    "\n",
    "**For your own good, do not use AI assistance to complete this assignment. You will be tested on this material in the first exam.** Using AI to complete this will not ultimately help you prepare for this upcoming evaluation. You also risk losing (all) points if your work is clearly AI-generated. \n",
    "\n",
    "If you get stuck, I recommend trying to work these problems out by hand. Drawing a picture or performing the arithmetic on paper can make things clearer. The lecture notes will also help you here. If you get really, *really* stuck, send me or a TA an email! \n",
    "\n",
    "### Submission\n",
    "Export the completed notebook as a PDF and submit on CMS prior to the deadline. **You are responsible for making sure that all cells are run and that the cell output is clearly visible prior to submission**. Double check your work before you click submit! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e42d0",
   "metadata": {},
   "source": [
    "## 1. Matrix multiplication (25 points)\n",
    "Implement dot product, matrix multiplication, and transpose in plain Python: **no Numpy/Pytorch allowed.** Pay attention to the types indicated in the docstrings. Don't worry too much about time or memory complexity (unless you want to): I just care that your implementation works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"\n",
    "    Dot product between two vectors \n",
    "\n",
    "    Args: \n",
    "        a: a n-dim vector (list, 1d np array, 1d tensor...)\n",
    "        b: a n-dim vector (list, np array, tensor...)\n",
    "    \n",
    "    Returns: \n",
    "        c: Dot product of a and b (float)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    ...\n",
    "\n",
    "\n",
    "def mm(A, B): \n",
    "    \"\"\"\n",
    "    Matrix multiply.\n",
    "\n",
    "    Args: \n",
    "        A: n x m matrix (list of lists, np array, tensor...)\n",
    "        B: m x p matrix (list of lists, np array, tensor...)\n",
    "\n",
    "    Returns:\n",
    "        C: A @ B \n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Use the dot product function above to help you\n",
    "    ...\n",
    "\n",
    "def transpose(A): \n",
    "    \"\"\"\n",
    "    Transpose a matrix. (You will need this later.) \n",
    "\n",
    "    Args: \n",
    "        A: a n x m matrix A (list of lists, np array, tensor...)\n",
    "\n",
    "    Returns:\n",
    "        A^T: a m x n matrix, with rows and columns of A swapped.  \n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb27b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests for the functions above to help you\n",
    "# Feel free to add more if you like\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "a_test = np.random.randn(10)\n",
    "b_test = np.random.randn(10)\n",
    "true_dot = (a_test * b_test).sum()\n",
    "your_dot = dot(a_test.tolist(), b_test.tolist())\n",
    "\n",
    "assert np.allclose(true_dot, your_dot)\n",
    "\n",
    "A_test = np.random.randn(4, 10)\n",
    "B_test = np.random.randn(10, 3)\n",
    "\n",
    "true_mm = A_test @ B_test \n",
    "your_mm = mm(A_test.tolist(), B_test.tolist())\n",
    "\n",
    "assert np.allclose(true_mm, your_mm)\n",
    "\n",
    "assert np.allclose(A_test.T, transpose(A_test.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93a176",
   "metadata": {},
   "source": [
    "## 2. Sigmoid and ReLU (25 points)\n",
    "\n",
    "Complete the functions below. I have imported `exp` for you, but you should not use any other imports here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108411ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    The sigmoid function. \n",
    "\n",
    "    Args: \n",
    "        x: a float, list of floats, list-of-lists of floats, np array, tensor, ...\n",
    "    Returns: \n",
    "        sigmoid(x)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    ...\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    The ReLU (rectified linear unit) function. \n",
    "    \n",
    "    Args:\n",
    "        x: a float, list of floats, list-of-lists of floats, np array, tensor...\n",
    "    Returns: \n",
    "        ReLU(x)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30df33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some additional tests to help you\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "assert np.allclose(F.sigmoid(1.0), sigmoid(1.0))\n",
    "\n",
    "test_a = np.random.randn(10)\n",
    "test_A = np.random.randn((10, 10))\n",
    "\n",
    "true_sigmoid = F.sigmoid(test_a)\n",
    "your_sigmoid = sigmoid(test_a.tolist())\n",
    "\n",
    "assert np.allclose(true_sigmoid, your_sigmoid)\n",
    "\n",
    "true_sigmoid = F.sigmoid(test_A)\n",
    "your_sigmoid = sigmoid(test_A)\n",
    "\n",
    "assert np.allclose(true_sigmoid, your_sigmoid)\n",
    "\n",
    "assert np.allclose(F.relu(-1.0), relu(-1.0))\n",
    "assert np.allclose(F.relu(1.0), relu(1.0))\n",
    "\n",
    "true_relu = F.relu(test_a)\n",
    "your_relu = relu(test_a)\n",
    "\n",
    "assert np.all_close(true_relu, your_relu)\n",
    "\n",
    "true_relu = F.relu(test_A)\n",
    "your_relu = relu(test_A)\n",
    "\n",
    "assert np.allclose(true_relu, your_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d8fdb",
   "metadata": {},
   "source": [
    "## 3. Building a Fully Connected Layer (25 points)\n",
    "Here, implement a single linear layer: \n",
    "$$ y = xW^\\intercal + b $$\n",
    "where $x$ is your input, $W$ is your weight matrix, and $b$ is your bias vector. \n",
    "\n",
    "You may not use any Pytorch or Numpy here: for matmul, use the function you wrote earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    A mock fully-connected linear layer: \n",
    "    y = xW^T + b\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, weight=None, bias=None):\n",
    "        \"\"\"\n",
    "        Initialize a layer that accepts in_features in and outputs \n",
    "        out_features. If provided, initialize the weight matrix and bias vector\n",
    "        with provided values. Otherwise, set all weights and bias to 0.0. \n",
    "\n",
    "        Args: \n",
    "            in_features: int\n",
    "            out_features: int\n",
    "            weight: Optional out_features x in_features matrix\n",
    "                (list-of-lists). \n",
    "            bias: Optional bias vector of size out_features. \n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Your code here!\n",
    "        if weight: \n",
    "            self.weight = weight \n",
    "        else: \n",
    "            ...\n",
    "\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            ... \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x: Input array-like with shape (*, self.in_features)\n",
    "        \n",
    "        Returns: \n",
    "            y: Output array-like with shape (*, self.out_features)\n",
    "        \"\"\"\n",
    "        # Your code here!\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de6166",
   "metadata": {},
   "source": [
    "Now, demonstrate that your layer behaves identically to Pytorch's `F.linear` in the cell below (check the Pytorch docs to see how to use this functional version of `nn.Linear`). You can do this by passing the same weights and bias term to both implementations when processing identical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a654a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7772d",
   "metadata": {},
   "source": [
    "## 4. Multilayer Perceptron (25 Points)\n",
    "\n",
    "Construct an MLP using the classes/functions you've written above. The MLP should have the following structure:\n",
    "\n",
    "- Linear layer w/ 8 input features, 16 output features\n",
    "- ReLU\n",
    "- Linear layer with ? input features, 32 output features\n",
    "- ReLU\n",
    "- Linear layer with ? input features, 16 output features\n",
    "- ReLU\n",
    "- Linear layer with ? input features, 8 output features\n",
    "- Sigmoid \n",
    "\n",
    "You should be able to determine the values of the \"?\"s above. \n",
    "\n",
    "As with all prior sections, do not use any imports, Numpy/Pytorch functions, etc. What you've already written will be sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b83c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP: \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        An MLP with layers and activation functions as described\n",
    "        above. \n",
    "        \"\"\"\n",
    "        ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the MLP. \n",
    "        \"\"\"\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-4940-llms-s26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
